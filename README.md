For, understanding webRTC, we explore the userMedia Interface.

When developing for the web, the WebRTC standard provides APIs for accessing cameras and microphones connected to the computer or smartphone. These devices are commonly referred to as Media Devices and can be accessed with JavaScript through the navigator.mediaDevices object, which implements the MediaDevices interface. From this object we can enumerate all connected devices, listen for device changes (when a device is connected or disconnected), and open a device to retrieve a Media Stream.

The most common way this is used is through the function getUserMedia(), which returns a promise that will resolve to a MediaStream for the matching media devices. This function takes a single MediaStreamConstraints object that specifies the requirements that we have.


https://github.com/bsilvers64/mediaDevices-Interface-showcase/assets/48654366/4dd0fcb2-c055-44f3-8744-916bfe39a12d


in our simple page we utilize the below methods -

# MediaDevices API Overview

The **MediaDevices** interface provides access to connected media input devices like cameras and microphones, as well as screen sharing. In essence, it lets you obtain access to any hardware source of media data.

The **MediaStream** interface of the **Media Capture and Streams API** represents a stream of media content. A stream consists of several tracks, such as video or audio tracks. Each track is specified as an instance of `MediaStreamTrack`.

We can access these tracks with the `getTracks()` method on the `mediaStream` object, for example. This contains 2 tracks, one for audio, and the other for video.

The **MediaDevices.getUserMedia()** method prompts the user for permission to use a media input, producing a `MediaStream` with tracks containing the requested types of media. It returns a Promise that resolves to a `MediaStream` object. If the user denies permission or matching media is not available, the promise is rejected with `NotAllowedError` or `NotFoundError` DOMException, respectively.

More here: [MediaDevices - MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices)

We have a function, `changeButtons()`, which changes the colors of the buttons.

We can change the video resolution on the front-end through much parsing and processing, but that can easily be done via making changes in the API. We have used constraints for the type of media to request in `getUserMedia`. So we will add our additional requirements in those constraints, like the resolution. And we can ask the individual tracks (our audio or video) what constraints are available. For example, here we get 2 objects listing supported constraints. The first is for the audio track, and the second is for the video track.

**MediaTrackConstraints** - a dictionary is used to describe a set of capabilities and the value or values each can take on. A constraints dictionary is passed into `applyConstraints()` to allow a script to establish a set of exact (required) values or ranges and/or preferred values or ranges of values for the track, and the most recently-requested set of custom constraints can be retrieved by calling `getConstraints()`. And, the **MediaTrackSupportedConstraints** dictionary establishes the list of constrainable properties recognized by the user agent or browser in its implementation of the `MediaStreamTrack` object.

We change the video aspect-ratio, height-width, framerate, etc., by `applyConstraints()` method after observing what constraints are supported by the particular browser.

## MediaRecorder

The **MediaStream Recording API**, sometimes referred to as the Media Recording API or the MediaRecorder API, is closely affiliated with the Media Capture and Streams API and the WebRTC API. The MediaStream Recording API makes it possible to capture the data generated by a MediaStream or HTMLMediaElement object for analysis, processing, or saving to disk. It's also surprisingly easy to work with.

The **MediaStream Recording API** consists of a single major interface, `MediaRecorder`, which does all the work of taking the data from a `MediaStream` and delivering it to you for processing. The data is delivered by a series of `dataavailable` events, already in the format you specify when creating the `MediaRecorder`. You can then process the data further or write it to a file as desired.

The **MediaRecorder** interface of the **MediaStream Recording API** provides functionality to easily record media. The `MediaRecorder()` constructor creates a new `MediaRecorder` object that will record a specified `MediaStream`. The object can optionally be configured to record using a specific media container (file type) and can specify the exact codec and codec configuration(s) to use by specifying the `codecs` parameter.

More info: [MediaRecorder - MDN Web Docs](https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder/MediaRecorder)

The `dataavailable` event is fired when the `MediaRecorder` delivers media data to your application for its use. The data is provided in a Blob object that contains the data. The `dataavailable` event fires off in 4 conditions; for our case, its `MediaRecorder.stop()` is called. The other 3 are mentioned [here](https://developer.mozilla.org/en-US/docs/Web/API/MediaRecorder/dataavailable_event).

So when a `stopRecording` event occurs on button press, it calls the `mediaRecorder.stop()` method which fires the `ondataavailable()` event which provides us our data.

We are giving our second video player tag’s src an object URL string of our blob data not a stream object.

## MediaDevices: `getDisplayMedia()` method

The `getDisplayMedia()` method of the **MediaDevices** interface prompts the user to select and grant permission to capture the contents of a display or portion thereof (such as a window) as a `MediaStream`. The resulting stream can then be recorded using the **MediaStream Recording API** or transmitted as part of a **WebRTC** session. The return value is a Promise that resolves to a `MediaStream` containing a video track whose contents come from a user-selected screen area, as well as an optional audio track. It is a different stream from the mic and the camera one.

## MediaDevices: `enumerateDevices()` method

The **MediaDevices** method `enumerateDevices()` requests a list of the currently available media input and output devices, such as microphones, cameras, headsets, and so forth. The returned Promise is resolved with an array of `MediaDeviceInfo` objects describing the devices. The returned list will omit any devices that are blocked by the document Permission Policy: microphone, camera, speaker-selection (for output devices), and so on. Access to particular non-default devices is also gated by the Permissions API, and the list will omit devices for which the user has not granted explicit permission. Return value - A Promise that is fulfilled with an array of `MediaDeviceInfo` objects. Each object in the array describes one of the available media input and output devices. The order is significant — the default capture devices will be listed first. Other than default devices, only devices for which permission has been granted are "available". If enumeration fails, the promise is rejected.

## HTMLMediaElement: `setSinkId()` method

To change the audio output device - The `HTMLMediaElement.setSinkId()` method of the **Audio Output Devices API** sets the ID of the audio device to use for output and returns a Promise. This only works when the application is permitted to use the specified device.


HTMLMediaElement: setSinkId() method
To change the audio output device -
The HTMLMediaElement.setSinkId() method of the Audio Output Devices API sets the ID of the audio device to use for output and returns a Promise.

This only works when the application is permitted to use the specified device.
